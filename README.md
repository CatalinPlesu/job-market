# Job Scraper TUI - Implementation Spec

## Configuration Files

**`settings.py`** - Hand-written user configuration
- LLM API credentials (key, endpoint, model)
- List of job site URLs to scrape
- Scraping settings (default delay, max parallel sites)

**`scraper_rules.json`** - AI-generated per-site rules
- CSS selectors for each site
- Pagination patterns
- Field mappings to standard schema
- Auto-generated by Menu #2, don't edit manually

## Database Schema

Single `jobs` table in SQLite with standardized fields:

```sql
CREATE TABLE jobs (
    id INTEGER PRIMARY KEY,
    scraped_at TIMESTAMP,
    site_name TEXT NOT NULL,
    url TEXT UNIQUE NOT NULL,
    posted_date TEXT,
    email TEXT,
    phone TEXT,
    
    -- Standard fields (mapped from all sites)
    title TEXT,
    company TEXT,
    employment_type TEXT, --full time, part time, internship , etc

    -- Location
    adress TEXT,
    -- 
    salary TEXT,
    -- 
    education TEXT,
    experience TEXT,
    
    descriptino TEXT,
);
```

## Menu Options

### 1. Print Config
Display contents of `settings.py` and `scraper_rules.json`

### 2. Generate Scraper Rules
Auto-generate `scraper_rules.json` using LLM:

**Process:**
1. Read API settings from `settings.py`
2. For each site URL:
   - Fetch 2-3 sample pages using `requests` + `BeautifulSoup`
   - Send HTML to LLM with prompt requesting:
     - **Job list selector**: CSS selector for all job cards on listing page
     - **Pagination pattern**: URL structure for page navigation (e.g., `?page={n}`)
     - **Field selectors**: Map site's HTML to standard schema fields (title, company, location, salary_text, posted_date, job_type, description)
     - **Extra fields**: Any site-specific data → `extra_data` JSON
3. Save mappings to `scraper_rules.json`

**Validation:** 
- Test selectors on 3 pages
- Require 80%+ of standard fields populated
- Show preview, ask user to confirm

### 3. Generate Debug JavaScript
Create browser console testing tool:

**Output:** `debug_js/{sitename}.js`

**Function:**
- Self-contained script that highlights scraped elements with color-coding
- One color per field type (title=blue, company=green, etc.)
- Copy/paste into browser console to verify selectors visually

### 4. Scrape Data
Main scraping workflow with two steps:

**Step 1: Collect Job URLs**
- Navigate pagination using pattern from `scraper_rules.json`
- Extract job post URLs from listing pages
- Stop when page returns 404 or "no results" error
- Store URLs in temporary list

**Step 2: Scrape Job Details**
- Visit each URL from Step 1
- Extract data using selectors from `scraper_rules.json`
- Map to standard schema fields
- Insert into `jobs` table (skip if URL exists)

**State Management:**
- Use `tinydb` for `state.json` progress cache
- Track per site: `{current_step, current_page, urls_collected, last_scraped_url}`
- Auto-resume on crash
- Delete `state.json` on successful completion

**Concurrency & Rate Limiting:**
- Scrape max 3 sites in parallel
- Respect `robots.txt` crawl-delay per site
- Default to 1 second delay if no rules specified
- Use `urllib.robotparser` to check disallowed paths

**Error Handling:**
- 3 retries with exponential backoff (1s, 2s, 4s)
- Log failures to `errors.log` with timestamp and URL
- Continue to next URL on persistent failure

**Deduplication:**
- Check if URL exists in database before scraping detail page
- Skip already-scraped jobs

### 5. Filter & Export
Query and export scraped data:
- Filter by keywords, location, date range, salary
- Export results to CSV or JSON
- Display summary statistics

### 6. View Results
Display scraped jobs:
- Show recent jobs in formatted terminal table
- Optional: Generate static HTML report with job listings

## Tech Stack

- **HTTP:** `requests`
- **HTML parsing:** `BeautifulSoup4`
- **Database:** `sqlite3` (built-in)
- **State cache:** `tinydb`
- **Robots.txt:** `urllib.robotparser`
- **Concurrency:** `concurrent.futures` or `asyncio`

## Error Cases to Handle

- LLM returns invalid selectors → validate and retry with refined prompt
- Site changes HTML structure → log error, mark site for re-generation
- Rate limiting/blocking → exponential backoff, respect retry-after headers
- Network failures → retry logic with timeout
- Partial data (missing fields) → insert with nulls, log warning
